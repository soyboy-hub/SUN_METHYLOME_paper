{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03846d-6478-49c5-a5d4-6b7dcec9122b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###functions\n",
    "import pandas as pd\n",
    "import pysam\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import pdist\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tempfile\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from intervaltree import Interval, IntervalTree\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def filter_te_by_coverage(gff3_file, bam_file, output_file, coverage_threshold=10):\n",
    "    gff = pd.read_csv(gff3_file, sep=\"\\t\", comment=\"#\", header=None,\n",
    "                      names=[\"seqid\",\"source\",\"type\",\"start\",\"end\",\"score\",\"strand\",\"phase\",\"attributes\"])\n",
    "    \n",
    "    def parse_attributes(attr_str):\n",
    "        d = {}\n",
    "        for item in attr_str.split(\";\"):\n",
    "            if \"=\" in item:\n",
    "                k, v = item.split(\"=\",1)\n",
    "                d[k] = v\n",
    "        return d\n",
    "    \n",
    "    gff['attr_dict'] = gff['attributes'].apply(parse_attributes)\n",
    "    \n",
    "    te_df = gff[gff['type'] == \"transposable_element\"].copy()\n",
    "    ltr_df = gff[gff['type'] == \"long_terminal_repeat\"].copy()\n",
    "    \n",
    "    bam = pysam.AlignmentFile(bam_file, \"rb\")\n",
    "    \n",
    "    def avg_coverage(seqid, start, end):\n",
    "        # pysam uses 0-based, half-open intervals\n",
    "        cov = bam.count_coverage(seqid, start-1, end)\n",
    "        # cov is tuple of 4 arrays (A,C,G,T), sum to get total coverage\n",
    "        total_cov = [sum(base) for base in zip(*cov)]\n",
    "        return sum(total_cov) / len(total_cov) if total_cov else 0\n",
    "    \n",
    "    te_df['avg_cov'] = te_df.apply(lambda row: avg_coverage(row['seqid'], row['start'], row['end']), axis=1)\n",
    "    \n",
    "    # Filter by coverage threshold\n",
    "    te_pass = te_df[te_df['avg_cov'] >= coverage_threshold]\n",
    "    \n",
    "    te_ids = set(te_pass['attr_dict'].apply(lambda x: x.get(\"ID\")))\n",
    "    ltr_pass = ltr_df[ltr_df['attr_dict'].apply(lambda x: x.get(\"Parent\") in te_ids)]\n",
    "    \n",
    "    filtered = pd.concat([te_pass, ltr_pass]).sort_values(['seqid','start'])\n",
    "    \n",
    "    def dict_to_attr(d):\n",
    "        return \";\".join([f\"{k}={v}\" for k,v in d.items()])\n",
    "    \n",
    "    filtered['attributes'] = filtered['attr_dict'].apply(dict_to_attr)\n",
    "    \n",
    "    filtered[['seqid','source','type','start','end','score','strand','phase','attributes']].to_csv(\n",
    "        output_file, sep=\"\\t\", header=False, index=False\n",
    "    )\n",
    "    \n",
    "    bam.close()\n",
    "\n",
    "def compute_te_methylation(gff3_file, bedmethyl_file, output_file,\n",
    "                           flank_size=2000, bins=50, strand_oriented=True):\n",
    "    if not bedmethyl_file.endswith(\".gz\"):\n",
    "        gz_file = bedmethyl_file + \".gz\"\n",
    "    else:\n",
    "        gz_file = bedmethyl_file\n",
    "    tbi_file = gz_file + \".tbi\"\n",
    "\n",
    "    if not os.path.exists(gz_file):\n",
    "        print(f\"[INFO] Compressing {bedmethyl_file} with bgzip...\")\n",
    "        subprocess.run([\"bgzip\", \"-c\", bedmethyl_file], stdout=open(gz_file, \"wb\"), check=True)\n",
    "\n",
    "    if not os.path.exists(tbi_file):\n",
    "        print(f\"[INFO] Indexing {gz_file} with tabix...\")\n",
    "        subprocess.run([\"tabix\", \"-p\", \"bed\", gz_file], check=True)\n",
    "\n",
    "    tbx = pysam.TabixFile(gz_file)\n",
    "\n",
    "    tes = []\n",
    "    with open(gff3_file) as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            chrom, source, feature, start, end, score, strand, phase, attrs = line.strip().split(\"\\t\")\n",
    "            start, end = int(start), int(end)\n",
    "            attr_dict = {kv.split(\"=\")[0]: kv.split(\"=\")[1] for kv in attrs.split(\";\") if \"=\" in kv}\n",
    "            te_id = attr_dict.get(\"ID\")\n",
    "            lineage = attr_dict.get(\"Lineage\", \"NA\")\n",
    "            tes.append((chrom, start, end, strand, te_id, lineage))\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for chrom, te_start, te_end, strand, te_id, lineage in tes:\n",
    "        if strand == \"+\":\n",
    "            upstream_start, upstream_end = max(0, te_start - flank_size), te_start\n",
    "            downstream_start, downstream_end = te_end, te_end + flank_size\n",
    "        else:\n",
    "            upstream_start, upstream_end = te_end, te_end + flank_size\n",
    "            downstream_start, downstream_end = max(0, te_start - flank_size), te_start\n",
    "\n",
    "        regions = [\n",
    "            (\"upstream\", upstream_start, upstream_end),\n",
    "            (\"TE\", te_start, te_end),\n",
    "            (\"downstream\", downstream_start, downstream_end)\n",
    "        ]\n",
    "\n",
    "        global_bin = 1\n",
    "\n",
    "        for pos_label, rstart, rend in regions:\n",
    "            if rend <= rstart:\n",
    "                continue\n",
    "\n",
    "            bin_edges = np.linspace(rstart, rend, bins+1, dtype=int)\n",
    "            bin_numbers = list(range(global_bin, global_bin + bins))\n",
    "\n",
    "            if strand == \"-\" and strand_oriented:\n",
    "                bin_numbers = bin_numbers[::-1]\n",
    "\n",
    "            for (bstart, bend, bin_num) in zip(bin_edges[:-1], bin_edges[1:], bin_numbers):\n",
    "                try:\n",
    "                    records = tbx.fetch(chrom, bstart, bend)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "                meth_vals = []\n",
    "                for rec in records:\n",
    "                    fields = rec.split(\"\\t\")\n",
    "                    percent = float(fields[10])\n",
    "                    pos = int(fields[1])\n",
    "                    if bstart <= pos < bend:\n",
    "                        meth_vals.append(percent)\n",
    "\n",
    "                avg_meth = np.mean(meth_vals) if meth_vals else 0.0\n",
    "                results.append([te_id, bin_num, avg_meth, lineage, pos_label])\n",
    "\n",
    "            global_bin += bins\n",
    "\n",
    "    df = pd.DataFrame(results, columns=[\"TE_ID\",\"bin_number\",\"average_methylation\",\"Lineage\",\"position\"])\n",
    "    if output_file.endswith(\".csv\"):\n",
    "        df.to_csv(output_file, index=False)\n",
    "    else:\n",
    "        df.to_csv(output_file, index=False, sep=\"\\t\")\n",
    "\n",
    "def merge_methylation_files(cg_file, chg_file, chh_file, output_file):\n",
    "    df_cg = pd.read_csv(cg_file, sep=\"\\t\")\n",
    "    df_cg[\"context\"] = \"CG\"\n",
    "\n",
    "    df_chg = pd.read_csv(chg_file, sep=\"\\t\")\n",
    "    df_chg[\"context\"] = \"CHG\"\n",
    "\n",
    "    df_chh = pd.read_csv(chh_file, sep=\"\\t\")\n",
    "    df_chh[\"context\"] = \"CHH\"\n",
    "\n",
    "    df_all = pd.concat([df_cg, df_chg, df_chh], axis=0, ignore_index=True)\n",
    "\n",
    "    df_all.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "\n",
    "def cluster_te_methylation_table(\n",
    "        infile, lineage, contexts=None,\n",
    "        metric='euclidean', cluster_method='average',\n",
    "        n_clusters=4, max_na=10, eval_range=(2,10),\n",
    "        save_clustered_file=\"clustered_TE_table.tsv\"):\n",
    "\n",
    "    df = pd.read_csv(infile, sep=\"\\t\")\n",
    "    df_te = df[(df['Lineage']==lineage) & (df['Position']==\"TE\")]\n",
    "    if df_te.empty:\n",
    "        raise ValueError(f\"No data for lineage {lineage} and Position==TE.\")\n",
    "\n",
    "    if contexts is None:\n",
    "        contexts = sorted(df_te['Context'].unique())\n",
    "    first_ctx = contexts[0]\n",
    "\n",
    "\n",
    "    df_first = df_te[df_te['Context']==first_ctx]\n",
    "    pivot = df_first.pivot(index='TE_ID', columns='Bin_num', values='Average_methylation_per_bin')\n",
    "\n",
    "    pivot = pivot[pivot.isna().sum(axis=1) <= max_na]\n",
    "    \n",
    "    pivot_filled = pivot.fillna(0)\n",
    "\n",
    "    first_col = pivot_filled.iloc[:,0].to_numpy()\n",
    "    pivot_filled = pivot_filled.loc[(pivot_filled.to_numpy() != first_col[:, None]).any(axis=1)]\n",
    "    if pivot_filled.empty:\n",
    "        raise ValueError(\"No TE_IDs left after filtering.\")\n",
    "\n",
    "    data_mat = pivot_filled.values\n",
    "    Z = hierarchy.linkage(pdist(data_mat, metric=metric), method=cluster_method)\n",
    "\n",
    "    cluster_labels = hierarchy.fcluster(Z, t=n_clusters, criterion='maxclust')\n",
    "    te_cluster_map = dict(zip(pivot_filled.index, cluster_labels))\n",
    "\n",
    "    wss_list = []\n",
    "    cluster_range = list(range(eval_range[0], eval_range[1]+1))\n",
    "    for k in cluster_range:\n",
    "        labels = hierarchy.fcluster(Z, t=k, criterion='maxclust')\n",
    "        wss = sum(((data_mat[np.where(labels==cid)[0],:] -\n",
    "                    data_mat[np.where(labels==cid)[0],:].mean(axis=0))**2).sum()\n",
    "                  for cid in np.unique(labels))\n",
    "        wss_list.append(wss)\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(cluster_range, wss_list, 'o-', color='blue')\n",
    "    plt.xlabel(\"Number of clusters (k)\")\n",
    "    plt.ylabel(\"Within-cluster sum of squares (WSS)\")\n",
    "    plt.title(f\"Elbow plot for TE methylation clustering ({lineage})\")\n",
    "    plt.show()\n",
    "\n",
    "    out_df = []\n",
    "    for ctx in contexts:\n",
    "        df_ctx = df_te[df_te['Context']==ctx].copy()\n",
    "        df_ctx = df_ctx[df_ctx['TE_ID'].isin(pivot_filled.index)]\n",
    "        df_ctx['cluster'] = df_ctx['TE_ID'].map(te_cluster_map)\n",
    "        out_df.append(df_ctx)\n",
    "\n",
    "    out_df = pd.concat(out_df, ignore_index=True)\n",
    "    out_df.to_csv(save_clustered_file, sep=\"\\t\", index=False)\n",
    "\n",
    "def adjast_table_full(clustered_TE_txt, full_unclust_txt, out_txt):\n",
    "    with open(clustered_TE_txt, 'r') as old, \\\n",
    "    open(full_unclust_txt, 'r') as old1, \\\n",
    "    open(out_txt, 'w') as new:\n",
    "        new.write('TE_ID\\tBin_num\\tAverage_methylation_per_bin\\tLineage\\tPosition\\tContext\\tcluster\\n')\n",
    "        dict_clusters = {}\n",
    "        for line in old:\n",
    "            if 'TE_ID' in line:\n",
    "                continue\n",
    "            line = line.strip().split('\\t')\n",
    "            dict_clusters[line[0]] = line[-1]\n",
    "        for line in old1:\n",
    "            if 'TE_ID' in line:\n",
    "                continue\n",
    "            line = line.strip().split('\\t')\n",
    "            if line[0] in dict_clusters:\n",
    "                apply_cluster = dict_clusters[line[0]]\n",
    "                tmp = '\\t'.join(line)\n",
    "                new_line = f'{tmp}\\t{apply_cluster}\\n'\n",
    "                new.write(new_line)\n",
    "    !rm {clustered_TE_txt}\n",
    "\n",
    "def parse_gff3_features(gff3_path: str):\n",
    "    features = []\n",
    "    with open(gff3_path) as fh:\n",
    "        for line in fh:\n",
    "            if line.startswith(\"#\") or not line.strip():\n",
    "                continue\n",
    "            cols = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "            if len(cols) < 9:\n",
    "                continue\n",
    "            seqid, source, ftype, start, end, score, strand, phase, attrs_s = cols[:9]\n",
    "            start = int(start)\n",
    "            end = int(end)\n",
    "            attrs = {}\n",
    "            for part in attrs_s.split(\";\"):\n",
    "                if \"=\" in part:\n",
    "                    k, v = part.split(\"=\", 1)\n",
    "                    attrs[k] = v\n",
    "            features.append({\n",
    "                \"seqid\": seqid,\n",
    "                \"source\": source,\n",
    "                \"type\": ftype,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"score\": score,\n",
    "                \"strand\": strand if strand in (\"+\", \"-\") else \"+\",\n",
    "                \"phase\": phase,\n",
    "                \"attrs\": attrs\n",
    "            })\n",
    "    return features\n",
    "\n",
    "def mafft_align_two_seqs(seq_a: Tuple[str,str], seq_b: Tuple[str,str], mafft_exec: str = \"mafft\", mafft_options: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    seq_a, seq_b: tuples (header, sequence)\n",
    "    Returns dict with aligned sequences under keys 'a_aln' and 'b_aln'.\n",
    "    Uses a temporary FASTA file and calls mafft --auto.\n",
    "    \"\"\"\n",
    "    if mafft_options is None:\n",
    "        mafft_options = [\"--auto\", \"--quiet\"]\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        in_fa = os.path.join(td, \"input.fa\")\n",
    "        out_fa = os.path.join(td, \"aligned.fa\")\n",
    "        with open(in_fa, \"w\") as fh:\n",
    "            fh.write(f\">{seq_a[0]}\\n\")\n",
    "            fh.write(seq_a[1] + \"\\n\")\n",
    "            fh.write(f\">{seq_b[0]}\\n\")\n",
    "            fh.write(seq_b[1] + \"\\n\")\n",
    "        cmd = [mafft_exec] + mafft_options + [in_fa]\n",
    "        try:\n",
    "            with open(out_fa, \"w\") as outf:\n",
    "                subprocess.run(cmd, check=True, stdout=outf, stderr=subprocess.DEVNULL)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            raise RuntimeError(f\"MAFFT failed: {e}\")\n",
    "        records = list(SeqIO.parse(out_fa, \"fasta\"))\n",
    "        if len(records) < 2:\n",
    "            raise RuntimeError(\"MAFFT output did not contain two sequences.\")\n",
    "        return {\"a_aln\": str(records[0].seq).upper(), \"b_aln\": str(records[1].seq).upper()}\n",
    "\n",
    "_TRANSITIONS = {(\"A\",\"G\"), (\"G\",\"A\"), (\"C\",\"T\"), (\"T\",\"C\")}\n",
    "\n",
    "def count_transitions_transversions(aln1: str, aln2: str) -> Tuple[int,int,int]:\n",
    "    assert len(aln1) == len(aln2)\n",
    "    n_ts = 0\n",
    "    n_tv = 0\n",
    "    n_valid = 0\n",
    "    for a,b in zip(aln1, aln2):\n",
    "        if a == \"-\" or b == \"-\":\n",
    "            continue\n",
    "        a_up = a.upper()\n",
    "        b_up = b.upper()\n",
    "        if a_up not in (\"A\",\"C\",\"G\",\"T\") or b_up not in (\"A\",\"C\",\"G\",\"T\"):\n",
    "            continue\n",
    "        if a_up == b_up:\n",
    "            n_valid += 1\n",
    "            continue\n",
    "        n_valid += 1\n",
    "        if (a_up, b_up) in _TRANSITIONS:\n",
    "            n_ts += 1\n",
    "        else:\n",
    "            n_tv += 1\n",
    "    return n_ts, n_tv, n_valid\n",
    "\n",
    "def compute_k2p_from_counts(n_ts: int, n_tv: int, n_sites: int) -> Optional[float]:\n",
    "    if n_sites <= 0:\n",
    "        return None\n",
    "    P = n_ts / n_sites\n",
    "    Q = n_tv / n_sites\n",
    "    # K2P formula: K = -1/2 * ln(1 - 2P - Q) - 1/4 * ln(1 - 2Q)\n",
    "    a = 1.0 - 2.0*P - Q\n",
    "    b = 1.0 - 2.0*Q\n",
    "    try:\n",
    "        if a > 0 and b > 0:\n",
    "            K = -0.5 * math.log(a) - 0.25 * math.log(b)\n",
    "            return K\n",
    "        else:\n",
    "            d = P + Q\n",
    "            if d >= 0 and d < 0.75:\n",
    "                K_jc = -3.0/4.0 * math.log(1 - (4.0/3.0)*d)\n",
    "                return K_jc\n",
    "            else:\n",
    "                return None\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def compute_ltr_k2p_and_age(genome_fasta: str,\n",
    "                            gff3_path: str,\n",
    "                            mutation_rate_per_year: float,\n",
    "                            mafft_exec: str = \"mafft\",\n",
    "                            mafft_options: Optional[List[str]] = None,\n",
    "                            lineage_filter: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "\n",
    "    if mafft_options is None:\n",
    "        mafft_options = [\"--auto\", \"--quiet\"]\n",
    "    genome = SeqIO.to_dict(SeqIO.parse(genome_fasta, \"fasta\"))\n",
    "    feats = parse_gff3_features(gff3_path)\n",
    "    te_features = {}\n",
    "    ltr_by_parent = {}\n",
    "    for f in feats:\n",
    "        ftype = f[\"type\"]\n",
    "        attrs = f[\"attrs\"]\n",
    "        if ftype == \"transposable_element\":\n",
    "            te_id = attrs.get(\"ID\")\n",
    "            if not te_id:\n",
    "                te_id = f\"{f['seqid']}:{f['start']}-{f['end']}\"\n",
    "            te_features[te_id] = f\n",
    "        elif ftype == \"long_terminal_repeat\":\n",
    "            parent = attrs.get(\"Parent\")\n",
    "            if not parent:\n",
    "                parent = attrs.get(\"Name\")\n",
    "            if not parent:\n",
    "                continue\n",
    "            ltr_by_parent.setdefault(parent, []).append(f)\n",
    "    rows = []\n",
    "    for te_id, te_feat in te_features.items():\n",
    "        if lineage_filter:\n",
    "            if te_feat[\"attrs\"].get(\"Lineage\") not in lineage_filter:\n",
    "                continue\n",
    "        ltrs = ltr_by_parent.get(te_id, [])\n",
    "        if len(ltrs) < 2:\n",
    "            rows.append({\n",
    "                \"TE_ID\": te_id,\n",
    "                \"seqid\": te_feat[\"seqid\"],\n",
    "                \"LTR1_coords\": None,\n",
    "                \"LTR2_coords\": None,\n",
    "                \"K2P\": None,\n",
    "                \"Age_Mya\": None,\n",
    "                \"n_sites\": 0,\n",
    "                \"n_ts\": 0,\n",
    "                \"n_tv\": 0,\n",
    "                \"note\": \"less_than_two_ltrs\"\n",
    "            })\n",
    "            continue\n",
    "        ltrs_sorted = sorted(ltrs, key=lambda x: x[\"start\"])\n",
    "        ltr1 = ltrs_sorted[0]\n",
    "        ltr2 = ltrs_sorted[-1]\n",
    "        seqid = te_feat[\"seqid\"]\n",
    "        if seqid not in genome:\n",
    "            rows.append({\n",
    "                \"TE_ID\": te_id,\n",
    "                \"seqid\": seqid,\n",
    "                \"LTR1_coords\": f\"{ltr1['start']}-{ltr1['end']}\",\n",
    "                \"LTR2_coords\": f\"{ltr2['start']}-{ltr2['end']}\",\n",
    "                \"K2P\": None,\n",
    "                \"Age_Mya\": None,\n",
    "                \"n_sites\": 0,\n",
    "                \"n_ts\": 0,\n",
    "                \"n_tv\": 0,\n",
    "                \"note\": \"seqid_not_in_fasta\"\n",
    "            })\n",
    "            continue\n",
    "        strand = te_feat.get(\"strand\", \"+\")\n",
    "        seq_ltr1 = genome[seqid].seq[ltr1[\"start\"] - 1 : ltr1[\"end\"]]\n",
    "        seq_ltr2 = genome[seqid].seq[ltr2[\"start\"] - 1 : ltr2[\"end\"]]\n",
    "        if strand == \"-\":\n",
    "            seq_ltr1 = seq_ltr1.reverse_complement()\n",
    "            seq_ltr2 = seq_ltr2.reverse_complement()\n",
    "        # convert to plain strings\n",
    "        s1 = str(seq_ltr1).upper()\n",
    "        s2 = str(seq_ltr2).upper()\n",
    "\n",
    "        try:\n",
    "            aln = mafft_align_two_seqs((f\"{te_id}_LTR1\", s1), (f\"{te_id}_LTR2\", s2), mafft_exec=mafft_exec, mafft_options=mafft_options)\n",
    "            a_aln = aln[\"a_aln\"]\n",
    "            b_aln = aln[\"b_aln\"]\n",
    "        except Exception as e:\n",
    "            rows.append({\n",
    "                \"TE_ID\": te_id,\n",
    "                \"seqid\": seqid,\n",
    "                \"LTR1_coords\": f\"{ltr1['start']}-{ltr1['end']}\",\n",
    "                \"LTR2_coords\": f\"{ltr2['start']}-{ltr2['end']}\",\n",
    "                \"K2P\": None,\n",
    "                \"Age_Mya\": None,\n",
    "                \"n_sites\": 0,\n",
    "                \"n_ts\": 0,\n",
    "                \"n_tv\": 0,\n",
    "                \"note\": f\"mafft_failed:{e}\"\n",
    "            })\n",
    "            continue\n",
    "            \n",
    "        n_ts, n_tv, n_sites = count_transitions_transversions(a_aln, b_aln)\n",
    "        K = compute_k2p_from_counts(n_ts, n_tv, n_sites)\n",
    "        if K is None:\n",
    "            age_mya = None\n",
    "            note = \"k2p_unestimable\"\n",
    "        else:\n",
    "            age_years = K / (2.0 * float(mutation_rate_per_year))\n",
    "            age_mya = age_years / 1e6\n",
    "            note = \"ok\"\n",
    "        rows.append({\n",
    "            \"TE_ID\": te_id,\n",
    "            \"seqid\": seqid,\n",
    "            \"LTR1_coords\": f\"{ltr1['start']}-{ltr1['end']}\",\n",
    "            \"LTR2_coords\": f\"{ltr2['start']}-{ltr2['end']}\",\n",
    "            \"K2P\": K,\n",
    "            \"Age_Mya\": age_mya,\n",
    "            \"n_sites\": n_sites,\n",
    "            \"n_ts\": n_ts,\n",
    "            \"n_tv\": n_tv,\n",
    "            \"note\": note\n",
    "        })\n",
    "    df = pd.DataFrame(rows, columns=[\"TE_ID\",\"seqid\",\"LTR1_coords\",\"LTR2_coords\",\"K2P\",\"Age_Mya\",\"n_sites\",\"n_ts\",\"n_tv\",\"note\"])\n",
    "    return df\n",
    "\n",
    "def intersect_genes_tes(gene_gff, te_gff, output_file):\n",
    "    def parse_gff(file, feature_type):\n",
    "        data = []\n",
    "        with open(file) as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"#\"):\n",
    "                    continue\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                if len(parts) < 9:\n",
    "                    continue\n",
    "                if parts[2] != feature_type:\n",
    "                    continue\n",
    "                chrom = parts[0]\n",
    "                start = int(parts[3])\n",
    "                end = int(parts[4])\n",
    "                attrs = {kv.split(\"=\")[0]: kv.split(\"=\")[1] for kv in parts[8].split(\";\") if \"=\" in kv}\n",
    "                ID = attrs.get(\"ID\", \"NA\")\n",
    "                data.append({\"chrom\": chrom, \"start\": start, \"end\": end, \"ID\": ID})\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    genes = parse_gff(gene_gff, \"gene\")\n",
    "    tes = parse_gff(te_gff, \"transposable_element\")\n",
    "\n",
    "    chrom_trees = {}\n",
    "    for chrom in genes['chrom'].unique():\n",
    "        tree = IntervalTree()\n",
    "        for _, row in genes[genes['chrom'] == chrom].iterrows():\n",
    "            tree[row['start']:row['end']+1] = row['ID']\n",
    "        chrom_trees[chrom] = tree\n",
    "\n",
    "    output = []\n",
    "    for _, te in tes.iterrows():\n",
    "        chrom = te['chrom']\n",
    "        te_start, te_end = te['start'], te['end']\n",
    "        te_id = te['ID']\n",
    "        if chrom in chrom_trees:\n",
    "            overlaps = chrom_trees[chrom].overlap(te_start, te_end+1)\n",
    "            if overlaps:\n",
    "                for gene in overlaps:\n",
    "                    output.append({\"TE_ID\": te_id, \"type\": \"genic\", \"gene\": gene.data})\n",
    "            else:\n",
    "                output.append({\"TE_ID\": te_id, \"type\": \"non_genic\", \"gene\": \"NA\"})\n",
    "        else:\n",
    "            output.append({\"TE_ID\": te_id, \"type\": \"non_genic\", \"gene\": \"NA\"})\n",
    "\n",
    "    out_df = pd.DataFrame(output)\n",
    "    out_df = out_df.drop_duplicates(subset=[\"TE_ID\", \"gene\"])\n",
    "    out_df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "\n",
    "def calculate_te_gene_distance_from_files(gene_gff: str, te_gff: str):\n",
    "    colnames = [\"seqid\", \"source\", \"type\", \"start\", \"end\", \"score\", \"strand\", \"phase\", \"attributes\"]\n",
    "\n",
    "    genes = pd.read_csv(gene_gff, sep=\"\\t\", names=colnames, comment=\"#\", dtype={\"seqid\": str})\n",
    "    tes = pd.read_csv(te_gff, sep=\"\\t\", names=colnames, comment=\"#\", dtype={\"seqid\": str})\n",
    "\n",
    "    genes = genes[genes[\"type\"] == \"gene\"].copy()\n",
    "    tes = tes[tes[\"type\"] == \"transposable_element\"].copy()\n",
    "\n",
    "    genes[\"gene_id\"] = genes[\"attributes\"].str.extract(r'ID=([^;]+)')\n",
    "    tes[\"te_id\"] = tes[\"attributes\"].str.extract(r'ID=([^;]+)')\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for _, te in tes.iterrows():\n",
    "        te_start, te_end = te[\"start\"], te[\"end\"]\n",
    "        seqid = te[\"seqid\"]\n",
    "\n",
    "        same_chr_genes = genes[genes[\"seqid\"] == seqid]\n",
    "\n",
    "        if same_chr_genes.empty:\n",
    "            results.append((te[\"te_id\"], None))\n",
    "            continue\n",
    "\n",
    "        distances = same_chr_genes.apply(\n",
    "            lambda g: 0 if (te_end >= g[\"start\"] and te_start <= g[\"end\"])\n",
    "            else min(abs(te_start - g[\"end\"]), abs(te_end - g[\"start\"])),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        min_distance = distances.min()\n",
    "        results.append((te[\"te_id\"], int(min_distance)))\n",
    "\n",
    "    result_df = pd.DataFrame(results, columns=[\"TE_ID\", \"distance_to_gene\"])\n",
    "    return result_df\n",
    "\n",
    "def calc_gc_percent(seq: str) -> float:\n",
    "    seq = seq.upper()\n",
    "    if len(seq) == 0:\n",
    "        return 0.0\n",
    "    g = seq.count(\"G\")\n",
    "    c = seq.count(\"C\")\n",
    "    return 100.0 * (g + c) / len(seq)\n",
    "\n",
    "def te_cg_content(gff3_path, reference_fasta, outfile):\n",
    "    genome = SeqIO.to_dict(SeqIO.parse(reference_fasta, \"fasta\"))\n",
    "    pat_id = re.compile(r'ID=([^;\\n]+)')\n",
    "    pat_parent = re.compile(r'Parent=([^;\\n]+)')\n",
    "\n",
    "    tes = {}\n",
    "    ltrs = defaultdict(list)\n",
    "\n",
    "    with open(gff3_path) as fh:\n",
    "        for ln in fh:\n",
    "            if ln.startswith(\"#\") or not ln.strip():\n",
    "                continue\n",
    "            parts = ln.strip().split(\"\\t\")\n",
    "            if len(parts) < 9:\n",
    "                continue\n",
    "            seqid, _, ftype, start_s, end_s, _, _, _, attrs = parts\n",
    "            start, end = int(start_s), int(end_s)\n",
    "            m_id = pat_id.search(attrs)\n",
    "            te_id = m_id.group(1) if m_id else None\n",
    "            if ftype == \"transposable_element\" and te_id:\n",
    "                tes[te_id] = (seqid, start, end)\n",
    "            elif ftype == \"long_terminal_repeat\":\n",
    "                m_parent = pat_parent.search(attrs)\n",
    "                parent = m_parent.group(1) if m_parent else None\n",
    "                if parent:\n",
    "                    ltrs[parent].append((start, end))\n",
    "\n",
    "    with open(outfile, \"w\") as outf:\n",
    "        outf.write(\"TE_ID\\tCG_whole\\tCG_body\\tCG_LTR\\n\")\n",
    "\n",
    "        for te_id, (seqid, start, end) in tes.items():\n",
    "            seq = str(genome[seqid].seq[start-1:end])\n",
    "            cg_whole = calc_gc_percent(seq)\n",
    "            if te_id in ltrs and len(ltrs[te_id]) >= 2:\n",
    "                ltr1, ltr2 = sorted(ltrs[te_id])[:2]\n",
    "                ltr_seq1 = str(genome[seqid].seq[ltr1[0]-1:ltr1[1]])\n",
    "                ltr_seq2 = str(genome[seqid].seq[ltr2[0]-1:ltr2[1]])\n",
    "                cg_ltr = (calc_gc_percent(ltr_seq1) + calc_gc_percent(ltr_seq2)) / 2.0\n",
    "                mask = [True]*(end-start+1)\n",
    "                for s,e in [ltr1,ltr2]:\n",
    "                    for i in range(s-start, e-start+1):\n",
    "                        if 0 <= i < len(mask):\n",
    "                            mask[i] = False\n",
    "                body_seq = ''.join(b for i,b in enumerate(seq) if mask[i])\n",
    "                cg_body = calc_gc_percent(body_seq)\n",
    "            else:\n",
    "                cg_ltr, cg_body = 0.0, 0.0\n",
    "\n",
    "            outf.write(f\"{te_id}\\t{round(cg_whole,2)}\\t{round(cg_body,2)}\\t{round(cg_ltr,2)}\\n\")\n",
    "\n",
    "def compute_locus_methylation(bed_file, bedmethyl_file, output_prefix, bin_size=100):\n",
    "    if not bedmethyl_file.endswith(\".gz\"):\n",
    "        gz_file = bedmethyl_file + \".gz\"\n",
    "    else:\n",
    "        gz_file = bedmethyl_file\n",
    "    tbi_file = gz_file + \".tbi\"\n",
    "\n",
    "    if not os.path.exists(gz_file):\n",
    "        print(f\"[INFO] Compressing {bedmethyl_file}...\")\n",
    "        subprocess.run([\"bgzip\", \"-c\", bedmethyl_file], stdout=open(gz_file, \"wb\"), check=True)\n",
    "\n",
    "    if not os.path.exists(tbi_file):\n",
    "        print(f\"[INFO] Indexing {gz_file}...\")\n",
    "        subprocess.run([\"tabix\", \"-p\", \"bed\", gz_file], check=True)\n",
    "\n",
    "    tbx = pysam.TabixFile(gz_file)\n",
    "\n",
    "    # --- Load loci BED ---\n",
    "    loci = pd.read_csv(bed_file, sep=\"\\t\", header=None, names=[\"chrom\", \"start\", \"end\", \"name\"])\n",
    "\n",
    "    results = []\n",
    "\n",
    "    def region_methylation(chrom, start, end):\n",
    "        meth_vals = []\n",
    "        try:\n",
    "            for rec in tbx.fetch(chrom, start, end):\n",
    "                fields = rec.split(\"\\t\")\n",
    "                try:\n",
    "                    percent = float(fields[10])  # methylation %\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                pos = int(fields[1])\n",
    "                if start <= pos < end:\n",
    "                    meth_vals.append(percent)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "        return np.mean(meth_vals) if meth_vals else np.nan\n",
    "\n",
    "    for _, row in loci.iterrows():\n",
    "        chrom, start, end, name = row[\"chrom\"], row[\"start\"], row[\"end\"], row[\"name\"]\n",
    "        bins = np.arange(start, end, bin_size)\n",
    "        for i, bstart in enumerate(bins, 1):\n",
    "            bend = min(bstart + bin_size, end)\n",
    "            avg_meth = region_methylation(chrom, bstart, bend)\n",
    "            results.append([name, chrom, bstart, bend, i, avg_meth])\n",
    "\n",
    "    out_df = pd.DataFrame(results, columns=[\"locus_name\", \"chrom\", \"start\", \"end\", \"bin\", \"avg_methylation\"])\n",
    "    out_df[\"avg_methylation\"] = out_df[\"avg_methylation\"].fillna(0.0)\n",
    "    outfile = f\"{output_prefix}_binned_methylation.txt\"\n",
    "    out_df.to_csv(outfile, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a85cf7-8931-4b8f-9acc-dd088d7d720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering out supplementary, secondary alignments and unmapped reads, sorting and indexing\n",
    "!samtools view -b -@ 100 -F 0x904 aligned.bam > \\\n",
    "aligned.primary.bam\n",
    "!samtools sort -@ 100 -o aligned.primary.sort.bam \\\n",
    "aligned.primary.bam\n",
    "!samtools index -@ 100 aligned.primary.sort.bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e67470-630e-4b40-909a-967974bbf2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#methylation calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a148b-33ab-4b2d-84cf-98715a6fcd97",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reference genome context calling\n",
    "#CG\n",
    "!modkit motif bed GCF_002127325.2_HanXRQr2.0-SUNRISE_genomic.fna CG 0 > \\\n",
    "SUNRISE_genomic.CG.bed\n",
    "#CHG\n",
    "!modkit motif bed GCF_002127325.2_HanXRQr2.0-SUNRISE_genomic.fna CHG 0 > \\\n",
    "SUNRISE_genomic.CHG.bed\n",
    "#CHH\n",
    "!modkit motif bed GCF_002127325.2_HanXRQr2.0-SUNRISE_genomic.fna CHH 0 > \\\n",
    "SUNRISE_genomic.CHH.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de15833-71e0-4f3b-8dfe-0d7ceaa350ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mC calling\n",
    "!modkit pileup -t 100 --filter-threshold C:0.7 --ignore h \\\n",
    "aligned.primary.sort.bam \\\n",
    "aligned.primary.meth.bed\n",
    "#filtering out mC calls with less than 3 Nmod\n",
    "!awk '{if ($10 >= 3) print $0}' aligned.primary.meth.bed > \\\n",
    "aligned.primary.meth.filtered.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afc02f0-203e-4f96-acdf-1f9c412d1e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#methylation in contexts calling\n",
    "!bedtools intersect -sorted -a aligned.primary.meth.filtered.bed \\\n",
    "-b SUNRISE_genomic.CG.bed > \\\n",
    "SUNRISE_genomic.CG.meth.bed\n",
    "!bedtools intersect -sorted -a aligned.primary.meth.filtered.bed \\\n",
    "-b SUNRISE_genomic.CHG.bed > \\\n",
    "SUNRISE_genomic.CHG.meth.bed\n",
    "!bedtools intersect -sorted -a aligned.primary.meth.filtered.bed \\\n",
    "-b SUNRISE_genomic.CHH.bed > \\\n",
    "SUNRISE_genomic.CHH.meth.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b85732-dbe6-4851-8459-6228a553ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering by coverage\n",
    "filter_te_by_coverage('final_annotations/sunflower_LTR_RT.gff3', \n",
    "                      'aligned.primary.sort.bam', \n",
    "                      'sunflower_LTR_RT.filtered.gff3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ea57c7-65a7-4d04-b8da-fc8d03295c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "###methylation matrix calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2968eb4a-51fd-4658-8dbd-46297c707a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep 'dante_ltr\ttransposable_element' \\\n",
    "sunflower_LTR_RT.filtered.gff3 > \\\n",
    "sunflower_LTR_RT.filtered.transposable_element.gff3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94e7a22-c076-4ee4-8db0-b766c16ad30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_te_methylation(\n",
    "    gff3_file=\"sunflower_LTR_RT.filtered.transposable_element.gff3\",\n",
    "    bedmethyl_file=\"SUNRISE_genomic.CG.meth.bed\",\n",
    "output_file=\"sunflower_LTR_RT.CG.meth.transposable_element.txt\",\n",
    "    flank_size=2000,\n",
    "    bins=50\n",
    ")\n",
    "\n",
    "compute_te_methylation(\n",
    "    gff3_file=\"sunflower_LTR_RT.filtered.transposable_element.gff3\",\n",
    "    bedmethyl_file=\"SUNRISE_genomic.CHG.meth.bed\",\n",
    "output_file=\"sunflower_LTR_RT.CHG.meth.transposable_element.txt\",\n",
    "    flank_size=2000,\n",
    "    bins=50\n",
    ")\n",
    "\n",
    "compute_te_methylation(\n",
    "    gff3_file=\"sunflower_LTR_RT.filtered.transposable_element.gff3\",\n",
    "    bedmethyl_file=\"SUNRISE_genomic.CHH.meth.bed\",\n",
    "output_file=\"sunflower_LTR_RT.CHH.meth.transposable_element.txt\",\n",
    "    flank_size=2000,\n",
    "    bins=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d96536c-54b4-454f-9adb-cc094cd170df",
   "metadata": {},
   "outputs": [],
   "source": [
    "###calculate clusters per lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87342296-c284-40c5-8059-3b3f2eaca92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_methylation_files('sunflower_LTR_RT.CG.meth.transposable_element.txt', \n",
    "                        'sunflower_LTR_RT.CHG.meth.transposable_element.txt', \n",
    "                        'sunflower_LTR_RT.CHH.meth.transposable_element.txt', \n",
    "                        'sunflower_LTR_RT.all_contexts.meth.transposable_element.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0ae4ca-db1c-4c80-bcef-43eb3575bb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_n_clust = {\n",
    "    'Ale' : 2,\n",
    "    'Angela' : 2,\n",
    "    'Athila' : 3,\n",
    "    'Bianca' : 1,\n",
    "    'Ikeros' : 1,\n",
    "    'Ivana' : 2,\n",
    "    'Retand' : 3,\n",
    "    'SIRE' : 1,\n",
    "    'TAR' : 3,\n",
    "    'Tekay' : 4,\n",
    "    'Tork' : 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a3da50-81e8-4e13-808d-31d49e851549",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for lineage in dict_n_clust:\n",
    "    cluster_te_methylation_table(\n",
    "        'docker_sandbox/sources/sunflower_TE_annot/meth_binned_tables/sunflower_LTR_RT.all_contexts.meth1.transposable_element.txt', \n",
    "        lineage,\n",
    "        metric='correlation', cluster_method='average',\n",
    "        n_clusters=dict_n_clust[lineage], max_na=10, eval_range=(2,10),\n",
    "        save_clustered_file=f\"docker_sandbox/sources/sunflower_TE_annot/meth_binned_tables/sunflower_LTR_RT.all_contexts.chrom_type.{lineage}.txt.tmp\")\n",
    "        adjast_table_full(f'sunflower_LTR_RT.all_contexts.chrom_type.{lineage}.txt.tmp', \n",
    "                           'sunflower_LTR_RT.all_contexts.meth.transposable_element.txt', \n",
    "                          f'sunflower_LTR_RT.all_contexts.chrom_type.{lineage}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96379144-8ddc-4336-8d1b-c721d499dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_n_clust_ara = {\n",
    "    'Ale' : 3\n",
    "    'Tork' : 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99384824-2dc0-4e0e-b12e-ae229efca317",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lineage in dict_n_clust_ara:\n",
    "    cluster_te_methylation_table(\n",
    "        'docker_sandbox/sources/sunflower_TE_annot/AT_WT/SQK-NBD114-96_barcode24.all.fix.txt', \n",
    "        lineage,\n",
    "        metric='euclidean', cluster_method='ward',\n",
    "        n_clusters=dict_n_clust_ara[lineage], max_na=10, eval_range=(2,10),\n",
    "        save_clustered_file=f\"docker_sandbox/sources/sunflower_TE_annot/AT_WT/SQK-NBD114-96_barcode24.all.{lineage}.txt.tmp\")\n",
    "        adjast_table_full(f'docker_sandbox/sources/sunflower_TE_annot/AT_WT/SQK-NBD114-96_barcode24.all.{lineage}.txt.tmp', \n",
    "                           'docker_sandbox/sources/sunflower_TE_annot/AT_WT/SQK-NBD114-96_barcode24.all.fix.txt', \n",
    "                          f'docker_sandbox/sources/sunflower_TE_annot/AT_WT/SQK-NBD114-96_barcode24.all.{lineage}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c29c1-f99c-4e46-9a5e-b89d0a9986fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d70762c-c21d-4cd7-b91b-9860cfe79b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute methylation levels per specific region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c7fc3-a15d-4f30-879d-df8c61c9345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_locus_methylation('selected.bed',\n",
    "                          'SUNRISE_genomic.CG.meth1.bed', \n",
    "                          \"selected.CG.100bp.txt\")\n",
    "compute_locus_methylation('selected.bed',\n",
    "                          'SUNRISE_genomic.CHG.meth1.bed', \n",
    "                          \"selected.CHG.100bp.txt\")\n",
    "compute_locus_methylation('selected.bed',\n",
    "                          'SUNRISE_genomic.CHH.meth1.bed', \n",
    "                          \"selected.CHH.100bp.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
